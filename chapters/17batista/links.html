<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing in Csound [ Ref &amp; Links ]</title>		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body bgcolor="white" link="blue" vlink="purple">		<h3><font size="4" face="Times New Roman,Georgia,Times"><b>Reference Section</b></font></h3>		<p><font face="Times New Roman">This is a list of papers I found interesting and that were used as reference, or mentioned during the writing of this material.</font></p>		<p><font face="Times New Roman">F. Rosenblatt. 'The Perceptron: a probabilistic model for information storage and organization in the brain'. Psychological Review. 1958.</font></p>		<p><font face="Times New Roman">M. Minsky and S. Papert. 'Perceptrons'. MIT Press. Cambridge, 1969.</font></p>		<p><font face="Times New Roman">D. E. Rumelhart, G. E. Hinton and R. J. Williams. 'Learning Internal Representations by Error Propagation'. Chapter 8 of Rumelhart and McClelland 'Parallel Distributed Processing'. MIT Press. Cambridge, 1986.</font></p>		<p><font face="Times New Roman">S. E. Fahlman. 'An Empirical Study of Learning Speed in Back-Propagation Networks'. Technical Report CMU-CS-88-162, Carnegie Mellon University. Pittsburgh, 1988.</font></p>		<p><font face="Times New Roman">S. E. Fahlman and C. Lebiere. 'The Cascade-Correlation learning architecture'. Technical Report CMU-CS-90-100, Carnegie Mellon University. Pittsburgh, 1990.</font></p>		<p><font face="Times New Roman">E. M. Johannson, F. U. Dowla and D. M. Goodman. 'Backpropagation learning for multi-layer feed-forward neural networks using the conjugate gradient method'. Technical Report UCRL-JC-104850, Lawrence Livermore National Laboratory. 1990.</font></p>		<p><font face="Times New Roman">M. F. Moller. 'A scaled conjugate gradient algorithm for fast supervised learning'. IEEE Transactions on Systems, Man and Cybernetics. 1991.</font></p>		<p><font face="Times New Roman">M. Riedmiller and H. Braun. 'A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm', Proceedings of the IEEE International Conference on Neural Networks. San Francisco, 1993</font></p>		<p><font face="Times New Roman">M. C. Mozer. 'Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multiscale processing'. Connection Science. 1994.</font></p>		<p><font face="Times New Roman">J. Gibb. 'The Back Propagation Family Album'. Technical Report C/TR96-05, Macquarie University. Australia 1996.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font size="4" face="Times New Roman,Georgia,Times"><b>Links section</b></font></p>		<p><font face="Times New Roman">There are vast resources on-line, for those who want to explore further. A mere websearch using 'neural net' should convince you of that. Many researchers and organizations are available online, and you can download most relevant papers in postscript format.</font></p>		<p><font face="Times New Roman">Here are some of my favorite sites:<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Donald Tveter's Pages</b></font></p>		<p><font face="Times New Roman">This is a very comprehensive site, put together by someone who obviously loves neural networks and knows a lot about them. His 'Backpropagator's Review' gives you links to a large number of interesting papers, concerning all sorts of back-prop-based learning schemes. There's also a short back-prop explanation written by himself, most suited for beginners. Additionally you can check Tveter's interesting book 'The Pattern Recognition Basis of AI' and download some online-only chapters. The code examples he used during the research for the book are freely available. Donald Tveter also offered some helpful comments, during the writing of this material.</font></p>		<p><font face="Times New Roman">Be sure to check it all out at : <a href="http://www.dontveter.com/">http://www.dontveter.com</a><br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>The comp.ai.neural-nets FAQ</b></font></p>		<p><font face="Times New Roman">This is a very interesting FAQ, from the <a href="news:comp.ai.neural-nets">news:comp.ai.neural-nets</a> newsgroup. It answers most of the beginner's questions, as well as some of the advanced ones. If you want to learn more about neural nets, be sure to check this FAQ. The latest version can be found at <a href="ftp://ftp.sas.com/pub/neural/FAQ.html">ftp://ftp.sas.com/pub/neural/FAQ.html</a>.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Ohio State Neuroprose Archive</b></font></p>		<p><font face="Times New Roman">Here you'll find a very large collection of neural related papers. You can access it by FTP at <a href="ftp://archive.cis.ohio-state.edu/pub/neuroprose/">ftp://archive.cis.ohio-state.edu/pub/neuroprose</a>, and it is also mirrored at <a href="ftp://ftp.funet.fi/pub/sci/neural/neuroprose/">ftp://ftp.funet.fi/pub/sci/neural/neuroprose</a>. Paper names start with the author name, so its easy to locate them if you know who the author is.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Connectionist Archive Site (Carnegie Mellon University)</b></font></p>		<p><font face="Times New Roman">The Carnegie Mellon University archive, at <a href="http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/connect/connect-archives/">http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/connect/connect-archives</a>.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Neural Information Processing Systems 1995</b></font></p>		<p><font face="Times New Roman">The papers for the NIPS'95 are found here, at <a href="http://www.cs.cmu.edu/afs/cs/project/cnbc/nips/NIPS95/Papers.html">http://www.cs.cmu.edu/afs/cs/project/cnbc/nips/NIPS95/Papers.html</a>.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Neuroscience Search Engine</b></font></p>		<p><font face="Times New Roman">This is a very useful search engine for all kinds of neural matter, <a href="http://www.quasar.org/21698/knowledge/neuro.html">http://www.quasar.org/21698/knowledge/neuro.html</a>. You may also want to check Neurosciences on the Internet, at <a href="http://www.neuroguide.com/">http://www.neuroguide.com</a>.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Dr K. Gurney's Neural Net pages</b></font></p>		<p><font face="Times New Roman">Here you'll find explanations of several network models and learning rules. This site has a good coverage of subjects, that you might find useful. The address is <a href="http://www.shef.ac.uk/psychology/gurney/notes/contents.html">http://www.shef.ac.uk/psychology/gurney/notes/contents.html</a>.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Funet's Artificial Neural Networks Archive</b></font></p>		<p><font face="Times New Roman">A collection of neural related material, at <a href="http://www.funet.fi/pub/sci/neural">http://www.funet.fi/pub/sci/neural</a><br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>The Neural Processing Letters</b></font></p>		<p><font face="Times New Roman">Located at <a href="http://www.kluweronline.com/issn/1370-4621">http://www.kluweronline.com/issn/1370-4621</a>, this will allow you to download several interesting papers.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Neural Nets Bibliography</b></font></p>		<p><font face="Times New Roman">Available from the University of Arizona, at <a href="http://donkey.cs.arizona.edu:1994/bib/Neural/">http://donkey.cs.arizona.edu:1994/bib/Neural</a>, and another one from the OFAI Library, at <a href="http://www.ai.univie.ac.at/oefai/nn/conn_biblio.html">http://www.ai.univie.ac.at/oefai/nn/conn_biblio.html</a>.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>BibTeX Archive</b></font></p>		<p><font face="Times New Roman">The BibTeX Collection, located at <a href="#Anchor-index.html-49575" name="Anchor-index.html-49575">http://liinwww.ira.uka.de/bibliography/index.html#search</a> has databases of journals and conference proceedings. It offers a means of locating and citing articles, but the actual papers are not available.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman"><b>Conferences and Workshops</b></font></p>		<p><font face="Times New Roman">This page provides links to several conferences and workshops on the topic of neural networks. You can follow the links at <a href="http://www.ewh.ieee.org/tc/nnc/conferences/comprehensive.html">http://www.ewh.ieee.org/tc/nnc/conferences/comprehensive.html</a>.<br>		&nbsp;<br>		&nbsp;</font></p>		<p><font face="Times New Roman">		<hr>		</font></p>		<dir><font face="Times New Roman"><b>Previous:</b> <a href="whatnext.html">What next?</a></font>			<p><font face="Times New Roman"><b>Up:</b> <a href="index.html">Back to index</a></font>		</dir>	</body></html>