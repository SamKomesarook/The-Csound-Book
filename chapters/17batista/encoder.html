<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing in Csound [ Encoder ]</title>		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body bgcolor="white" link="blue" vlink="purple">		<h3><font size="4" face="Times New Roman,Georgia,Times"><b>Testing RProp with a Tight-encoder</b></font></h3>		<p><font face="Times New Roman">A tight-encoder is a network that has the same input and output size, and a single (smaller) hidden layer to provide the coding. We train the net with the same pattern present at both the input and the output, so the net learns auto-associations of the given patterns. These will consist of binary strings, each with a single 1 and remaining zeros (first pattern is 10000, second is 01000, third is 00100, etc). We will use a network of size 10, so we'll be training ten of this patterns. Riedmiller describes his results with a 10-5-10 network (ten input cells, five hidden cells, ten output cells), but I could only get it to converge with 6 hidden neurons, so we'll use a 10-6-10 network. The initialization of the weights plays a big role, and the rnd() function is perhaps not random enough (a random generator with a seed value, would allow us to test several start weights, and then maybe we can - on average - reach the described results).</font></p>		<p><font face="Times New Roman">This network will encode the 10 patterns using the values of its weights. Since the number of weights does not grow, this gives a way of performing data compression, which is the more effective, the fewer hidden neurons are used. Riedmiller describes 12-2-12 encoders, converging within an average 322 epochs (iterations) using RProp, where plain backprop didnt find a solution at all. You can try it with this instrument, although I admit I havent been able to recreate exactly the published results.</font></p>		<p><font face="Times New Roman">An interesting (and conclusive) exercise, is to compare this much more complicated problem and the speed of convergence using RProp, with our back-prop testing of the (much simpler) XOR problem. Backprop went way past 2,000 iterations, while with RProp, the average needed iterations lies well below a hundred, and if a solution doesnt appear in the, say, 500 first epochs, you might as well give up. Its this robustness of RProp, that leads me to think it can be used, in the highly demanding world of sound synthesis and processing.</font></p>		<p><font face="Times New Roman">		<hr>		</font></p>		<h3><font face="Times New Roman"><b>The Training Code</b></font></h3>		<p><font face="Times New Roman">The training process consists of two main steps:</font></p>		<ol>			<li><font face="Times New Roman">a <b>forward pass</b>, where the input pattern is presented to the net, and the response from that pattern is forwarded through all layers until the output (in our example, through the hidden and then through the output layer)</font>			<li><font face="Times New Roman">a <b>backward pass</b>, where the error for each connection is determined, starting from the output layer, where the difference between the actual (from the forward pass) and the desired target values is used to calculate it, and proceeding backwards, propagating the error values through the net to the preceding layers, in a similar fashion to the way the activation was propagated forward through the net, only backwards, and using the already computed error values from the next layer.</font>		</ol>		<p><font face="Times New Roman">Since I accepted the limitation of having just a single hidden layer, and also for clarity, I wasnt too worried in performing all layers using the same code loop. Thats why I process each layer in a separate loop during this instrument. At the end, the weight-update code shows what had to be done to compact this into a single main loop, running for each individual layer. For a multiple hidden layer network, most of this code could be used, but some caution must be used in the array accessing.</font></p>		<p><font face="Times New Roman">Lets see the code then, first of all the activation code, or forward-pass code:</font></p>		<p><font face="Times New Roman">We start by calculating the activation of all neurons in the hidden layer. For each neuron gather all inputs (io), times their connection weights (iw); then compute the sigmoid function for this added value (inet), and store it in the activation array (the 'ziw' of the ia value).</font></p>		<p><font size="2" face="Courier New">;activation of hidden layer</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu1 = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loop2:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">inet = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu0 = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loop3:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">io zir i_pat_i+ineu0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">iw zir (i_w)+ineu0*ihsz+ineu1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">inet = inet+iw*io</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu0 = ineu0+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineu0&lt;iisz) igoto loop3</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ia = 1/(1+exp(-inet))</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ziw ia, (i_a)+ineu1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu1 = ineu1+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineu1&lt;ihsz) igoto loop2</font></p>		<p><font face="Times New Roman">Then we calculate the activation of all neurons in the output layer, in a similar fashion, using the activation values of the neurons in the preceding (hidden) layer as the input values (io). Again, the added activation value (inet) is computed using the sigmoid, and the result stored in the activation array. But now we also check to see how well the net is performing (I know, we just started, but remember this code will be run many times, in an iterative process). For each output neuron, we compare the actual output value we just calculated, with the target, desired value, from the output pattern. If the distance between these two values is bigger than the predefined allowed error (ierng), then the neuron is not 'hitting' the desired spot, and we have what can be called 'a miss'. We use a variable imiss to count the number of badly recognized neurons for <b>all</b> patterns. Training stops when this value reaches zero, at the end of an iteration (meaning all patterns were entirely recognized).</font></p>		<p><font size="2" face="Courier New">;activation of output layer</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu1 = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loop4:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">inet = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu0 = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loop5:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">io zir (i_a)+ineu0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">iw zir (i_w+iisz*ihsz)+ineu0*iosz+ineu1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">inet = inet+iw*io</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu0 = ineu0+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineu0&lt;ihsz) igoto loop5</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ia = 1/(1+exp(-inet))</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ziw ia, (i_a+ihsz)+ineu1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">it zir i_pat_t+ineu1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (abs(ia-it)&lt;ierng) igoto yes</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">imiss = imiss+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">yes:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineu1 = ineu1+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineu1&lt;iosz) igoto loop4</font><font face="Times New Roman">&nbsp;</font></p>		<p><font face="Times New Roman">Now that we performed the forward pass, which updated the activation values for the whole net, we need to determine the error at the output, and transfer it backwards from the output layer, to the hidden layer. This backward pass, is the calculation of the error gradient, that we'll be needing as part of the learning rule. The actual formulas were introduced in the preceding section, and you may want to compare the code with what is presented there. Basically we want to calculate the dedw values, which will require the previous knowledge of the dedo values.</font></p>		<p><font face="Times New Roman">For the output layer, the dE/do value depends only on the difference between the desired and actual states. For each input connection of the neurons in this layer, the dE/dw value is then updated proportionally to the dE/do determined value, the derivative of the activation, and the input value on the particular connection.</font></p>		<p><font size="2" face="Courier New">;===============for the output layer=================</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuk = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loopk0:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">it zir i_pat_t+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ia zir (i_a+ihsz)+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">idedo = -(it-ia)</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ziw idedo, (i_dedo+ihsz)+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuj = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loopj0:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">io zir (i_a)+ineuj</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">idedw zir (i_dedw+iisz*ihsz)+ineuj*iosz+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">idedw = idedw+idedo*(ia*(1-ia))*io</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ziw idedw, (i_dedw+iisz*ihsz)+ineuj*iosz+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuj = ineuj+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineuj&lt;ihsz) igoto loopj0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuk = ineuk+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineuk&lt;iosz) igoto loopk0</font><font face="Times New Roman">&nbsp;</font></p>		<p><font face="Times New Roman">For the hidden layer, the dE/do values found for the output layer are back propagated, through their respective connections, to each hidden neuron, resulting in a combined error value. The dE/dw value for each connection of each hidden neuron, is then updated in pretty much the same way as for the output layer, using the calculated dE/do's.</font></p>		<p><font size="2" face="Courier New">;===============for the hidden layer=================</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineui = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loopi:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">io zir i_pat_i+ineui</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuj = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loopj:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">isoma = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuk = 0</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">loopk:</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">idedo zir (i_dedo+ihsz)+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">iw zir (i_w+iisz*ihsz)+ineuj*iosz+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ia zir (i_a+ihsz)+ineuk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">isoma = isoma+idedo*(ia*(1-ia))*iw</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuk = ineuk+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineuk&lt;iosz) igoto loopk</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">idedo = isoma</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ziw idedo, (i_dedo)+ineuj</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ia zir (i_a)+ineuj</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">idedw zir (i_dedw)+ineui*ihsz+ineuj</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">idedw = idedw+idedo*(ia*(1-ia))*io</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineuj = ineuj+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineuj&lt;ihsz) igoto loopj</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">ineui = ineui+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ineui&lt;iisz) igoto loopi</font></p>		<p><font face="Times New Roman">This completes the main processing loop for a single pattern. We will repeat these same steps for each and every pattern in the training set, and only after that we will perform any corrections to the weights. Notice the dE/dw values, which are needed for the weight changes, are being accumulated over all patterns.</font></p>		<p><font face="Times New Roman">We increment our pattern-counting variable (ipat), and restart processing:</font></p>		<p><font size="2" face="Courier New">ipat = ipat+1</font><font face="Courier New"><br>		</font><font size="2" face="Courier New">if (ipat&lt;inump) igoto newpat</font></p>		<p><font face="Times New Roman">After all patterns, and if the imiss variable still is zero (meaning there were no errors), we can stop training</font></p>		<p><font size="2" face="Courier New">if (imiss==0) igoto done</font></p>		<p><font face="Times New Roman">Otherwise, its time to update the weights, using the RProp algorithm (I leave the explanation of this portion to the curiosity of the readers; Riedmiller's papers are easily found on the net, and his explanations very clear).</font></p>		<p><font face="Times New Roman">Here's the complete orchestra file : <a href="instruments/encoder.orc">ENCODER.ORC</a></font></p>		<p><font face="Times New Roman">And the usual dummy score : <a href="instruments/dummy.sco">DUMMY.SCO</a><br>		&nbsp;&nbsp;</font></p>		<p><font face="Times New Roman">		<hr>		</font></p>		<dir><font face="Times New Roman"><b>Next : </b><a href="custom.html">A custom neural instrument</a></font>			<p><font face="Times New Roman"><b>Previous : </b><a href="rprop.html">Resilient back-propagation (RPROP)</a></font></p>			<p><font face="Times New Roman"><b>Up:</b> <a href="index.html">Back to index</a></font>		</dir>	</body></html>