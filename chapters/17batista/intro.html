<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing in Csound [ Intro ]</title>		<meta name="Template" content="C:\Programas\Microsoft Office\Modelos\P&aacute;ginas da Web\Assistente de P&aacute;gina da Web.wiz">	</head>	<body bgcolor="white" link="blue" vlink="purple">		<h3><font size="4" face="Times New Roman,Georgia,Times"><b>Introduction</b></font></h3>		<p><font face="Times New Roman">The work in the field of neural networks, started back in the early forties, when the first computers were far from being built. At the time McCulloch and Pitts were developing models of artificial neurons, based on their knowledge of neurophysiology and logical algebra. Their models were rather crude, using binary (either ON or OFF) neurons to calculate simple logic functions, like AND's and OR's. The fifties saw other pioneers continue this kind of investigation, using the first computer simulations, and at the end of that decade Rosenblatt presented the Perceptron. This was a machine, which was said to be inspired by the way the human eye operated, and that was able to associate patterns to specific output cells, thus performing some pattern recognition.</font></p>		<p><font face="Times New Roman">The Perceptron was a fairly simple device by todays standards, with just a layer of input neurons which would receive the pattern, and an output layer providing the result. These layers were linked by a number of weighted connections, that could be tuned in strength to achieve a solution. The training was done by a method known as hebbian learning (after Hebb, its developer), in which the connection's weights are updated based solely on its input and output values. This provided a method for single layer networks to learn, but although it could learn simple AND and OR functions, it could not be taught, for instance, a more complicated XOR function. This function required the presence of a hidden layer, one that wasnt directly connected to the input, neither to the output of the net. It was the lack of a training rule for hidden layers that led to the Minsky and Papert, 1969's disappointed book, describing the limitations of perceptrons, which, at the time, eliminated much of the funding and discouraged many researchers to proceed their work in the field.</font></p>		<p><font face="Times New Roman">During the seventies, research proceeded, at a slow pace, but with the eighties, several new paradigms were presented. Grossberg's work around Adaptive Resonance Theory (ART), in some aspects thought to operate similarly to the brain, and Kohonen's self-organization maps, a very popular unsupervised learning algorithm, are among the most interesting. The backpropagation rule was discovered independently a number of times, with the first progresses apparently emerging still in the fifties. In 1974 Werbos had already discovered and used the rule, but it was only in 1986 that the first paper on the subject, &quot;Learning Internal Representations by Error Propagation&quot; by Rumelhart, Hinton and Williams, was published, bringing it to public knowledge. What this paper described was a method for training networks with hidden layers. This kind of networks had the ability to construct their own internal representations of the training patterns, and that allowed them to provide much more complicated mappings, namely the solution for the exclusive-or paradigm.</font></p>		<p><font face="Times New Roman">This paper was one of the chapters of the 'Parallel Distributed Processing' (PDP) book, which along with Minsky and Papert's 'Perceptrons', remain some of the most interesting books on the subject, that I read in my university days. At that time I did program some simple perceptron nets, and managed to implement the back-propagation rule, using the described algorithm. But then there was another problem. Back propagation (as the algorithm is known, because of the way the error is back flushed through the hidden layers), is a very slow method. It is an iterative method, which means you have to run the same set of patterns, over and over again, causing the net to get a little bit closer to the solution, for each iteration. It may take thousands of iterations before the net arrives at a convenient solution (a minimum), and even that only when it converges to a solution at all. Furthermore, there isnt any guarantee, the net will arrive at an optimal, or even good solution!</font></p>		<p><font face="Times New Roman">Over the years, many enhancements and alterations to the basic algorithm have been proposed, in an effort to overcome its limitations. Some of the most popular are Fahlman's Cascade Correlation, Riedmilller's Resilient Backpropagation, and Johansson's Conjugate Gradient techniques. Several completely different approaches to neural computing have also been proposed, so many that its hard to keep track of every one of them. The field of neural computing is more alive than ever, with lots of researchers pursuing different routes and publishing their results. The reference and links section will provide pointers to several papers, if you wish to build your knowledge on the subject. This paper will only focus on the simplest algorithms, namely hebbian learning and back-propagation, with some emphasis placed on resilient backpropagation as well.</font></p>		<h3><font face="Times New Roman"><b>What Can Neural Nets Do?</b></font></h3>		<p><font face="Times New Roman">Neural nets can provide solution for cases where the data is uncertain, or deterministic rules are hard to apply but there is a lot of empirical data available. They can be a wonderful &quot;doctor&quot;, trained by the presentation of many symptoms and diagnosis pairs, which later presents a correct medical opinion when questioned with new cases. In theory, you could train a neural net with all of J. S. Bach's compositions, and then have it generate some new compositions using the same stylistic devices of the master (in fact this has been tried, check Mozer's work in the reference section), but there we must address several questions, with such an ambitious task, and the required architecture is very complex, and computationally expensive, making it rather impossible to code in Csound.</font></p>		<p><font face="Times New Roman">They are great at data encoding. The information learned by the net, is contained in the values of the connection weights between neurons. This means this array of values (which does not vary in size during training, or if it does it is as part of the algorithm) contains the information for all patterns trained and learned, the size of which is often substantially larger than the size of the connection weight's array.</font></p>		<p><font face="Times New Roman">But the solutions arrived at, are often not optimal, and furthermore, the fitness of the solution cant be determined by the net. So its rather unpredictable in results. Also, the arriving at a solution (converging) of the net, is highly dependant of the net structure, training algorithm, and training data. So solutions must be sharply tailored to particular needs. Unfortunately, since the global knowledge about the subject is still building, and particularly in the musical field, experimental data is scarce, a long process of trial-and-error must be performed to, hopefully, arrive at a not so bad solution.</font></p>		<h3><font face="Times New Roman"><b>How Can Neural Nets be Used with Csound ?</b></font></h3>		<p><font face="Times New Roman">Possible applications may be: 1) to extract significant features, from a set of examples, possibly in order to apply these features to other data, or generate similar new data. 2) extracting a function representation y = f (x) from a set of example (x; y) pairs, which may be used to mimic functions we dont know, or that are hard to calculate otherwise. 3) learning to predict the value of a variable, based on the past occurrences of this variable, which might be used for sample generation based on the time behavior of a training set of samples, but also for MIDI composition, where the training examples could be sequences of notes or chords. Theoretically all this applications can be implemented with some degree of complexity, and more or less elaborate architectures, but in practice, as we'll see, some of the more complex (and more interesting) applications, seem impracticable at least within Csound orchestra definitions, due to the extraordinary amount of calculations that distributed processing requires. Maybe the design of specific ugens, or Csound utilities, can overcome this practical limitation. Alternatively, RISC machines or UNIX mainframes, may afford the additional processing required, but thats something most of us dont posses, so lets remain conscious of the limitations.</font></p>		<p><font face="Times New Roman">		<hr>		</font></p>		<dir><font face="Times New Roman"><b>Next : </b><a href="neuron.html">Neuron basics</a></font>			<p><font face="Times New Roman"><b>Previous : </b><a href="summary.html">Summary</a></font></p>			<p><font face="Times New Roman"><b>Up:</b> <a href="index.html">Back to index</a></font>		</dir>	</body></html>