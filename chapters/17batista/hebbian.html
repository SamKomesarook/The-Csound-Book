<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing in Csound [ Hebbian ]</title>		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body bgcolor="white" link="blue" vlink="purple">		<h3><font size="4" face="Times New Roman,Georgia,Times"><b>Hebbian Learning</b></font></h3>		<p><font face="Times New Roman">Lets just focus on a very simple network, consisting of a single layer. To simplify explanations, lets keep the dimension of the input pattern, neuron layer and output pattern arrays fixed at two cells each</font></p>		<center>			<dir><font face="Times New Roman"><img src="figures/3.gif" height="206" width="287"></font>			</dir></center>		<p><font face="Times New Roman">Up until now, we've focused on the network and neuron structure, but we have said nothing about the way the net actually learns. Hopefully this section will begin to clarify that.</font></p>		<p><font face="Times New Roman">Lets start with the training material. To train the net, we will give it pairs of patterns, that will be presented respectively to the input and the output of the net. These patterns will be in this simple case, a sequence of binary numbers. So what we need here is a set of pairs of binary numbers, each with two digits, to present to the net's input and output.</font></p>		<p><font face="Times New Roman">The basic training procedure is</font></p>		<ol>			<li><font face="Times New Roman">Present the input patterns to the input of the net , and the desired outputs to the output of the net</font>			<li><font face="Times New Roman">Apply the training procedure so that the net evolves towards associating the two patterns</font>			<li><font face="Times New Roman">Repeat with another pair of patterns</font>		</ol>		<p><font face="Times New Roman">How does the net learn the association between the input and output patterns? By correcting each neuron's weights, towards the desired response. What this means is that the net will take the pattern at the input, and forward it through each neuron till the output. This will result in a certain output pattern, depending on the current weights of the net, which will probably be different form the desired pattern we wanted to get on the output. Then, the training algorithm will correct the necessary weights, so that the output of the net becomes the desired pattern.</font></p>		<p><font face="Times New Roman">For instance, consider neuron n is outputting the value 1. And consider also that for the current training pattern, the output of this particular neuron should be 0. This means that this neuron is getting too much stimulus in its input. Since we cant change the input pattern to decrease the input stimulus, we must change the connection weights, weakening some of the connections so that the neuron doesnt get such a high stimulus, and does not fire (thus outputting the desired 0). The connections that will have to be weakened will be those that are receiving a 1 from the input pattern.</font></p>		<p><font face="Times New Roman">If the situation was reverse (outputting a 0 when the expected target was 1), then some of the connections would have to be strengthened, so that their input stimulus would be amplified and would cause the neuron to fire.</font></p>		<p><font face="Times New Roman">What we expect to reach, with the training process, is a correct mapping of all of the input patterns to the respective output patterns. The mapping will be solely provided by the network weights, so this provides a very effective way of information encoding:</font></p>		<p><font face="Times New Roman">If the input size is N, and the output size is M, then the whole weight array will take N*M values. If each pattern would have been recognized by a conventional program, it would have to store the whole pattern pairs, thus requiring N+M values per pattern pair. Now to store X patterns, a conventional program would require X*(N+M) values, while the neural network just uses the same N*M values in memory regardless of the number of patterns. In fact, the constraint here is the number of patterns the network can learn (which in the simple single-layer net we're studying is about three...)</font></p>		<p><font face="Times New Roman">The learning rule, is the process by which the network algorithm corrects every network connection strength, for each of the training pairs presented. This rule can be very complex, but for single-layer networks, a form of training, called hebbian learning, can be successfully applied. The simplicity of this rule, stems from the fact that only the activation values on each end of the connection, need to be taken in consideration to determine the weight change.</font></p>		<p><font face="Times New Roman">Suppose the input of the net, consists of a vector 'in', made of individual binary values, and the desired output of the net is also a vector 'out', consisting of individual binary digits. Then the correction amount for the weight between the input cell number i and the output cell j, is given by</font></p>		<center>			<dir><font face="Times New Roman"><img src="figures/form05.gif" height="38" width="288"></font>			</dir></center>		<p><font face="Times New Roman">What the formula does, is to convert the binary values at the input and output, from either 0 or 1 (binary), to either -1 or +1 (bipolar) - the 2*x-1, factor does that - and then multiplies this new input and output values. The result amount is then added to the corresponding weight.</font></p>		<p><font face="Times New Roman">Returning to the simple example depicted at the top of this page, this means that for the pattern pair consisting of input i[1], i[2], and output o[1], o[2], we would need to perform the following updates to the weight array,</font></p>		<p><font face="Times New Roman">w[1][1] = w[1][1] + (2*i[1]-1)*(2*o[1]-1)</font></p>		<p><font face="Times New Roman">w[1][2] = w[1][2] + (2*i[1]-1)*(2*o[2]-1)</font></p>		<p><font face="Times New Roman">w[2][1] = w[2][1] + (2*i[2]-1)*(2*o[1]-1)</font></p>		<p><font face="Times New Roman">w[2][2] = w[2][2] + (2*i[2]-1)*(2*o[2]-1)</font></p>		<p><font face="Times New Roman">in order to have the net learn this pattern pair.</font></p>		<p><font face="Times New Roman">One important issue is weight array initialization. Hebbian learning requires that the weight array is initialized with zeros (this is because the learning algorithm updates weights in a +1 or -1 basis, zero being the middle value).</font></p>		<p><font face="Times New Roman">Other algorithms like back propagation can have the weights initialized by a value between 0 and 1, or in a symmetric small range centered at zero, although there's a lot of discussion still going about the best weight initializations methods. In the back-propagation algorithms I present, the weights are initialized with random values in the range 0 to 1, or -1 to +1.</font></p>		<p><font face="Times New Roman">		<hr>		</font></p>		<dir><font face="Times New Roman"><b>Next : </b><a href="bam.html">An associative memory instrument</a></font>			<p><font face="Times New Roman"><b>Previous : </b><a href="stqs.html">First questions</a></font></p>			<p><font face="Times New Roman"><b>Up:</b> <a href="index.html">Back to index</a>&nbsp;</font>		</dir>	</body></html>