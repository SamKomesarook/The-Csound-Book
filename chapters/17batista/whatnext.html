<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing in Csound [ What Next? ]</title>		<meta name="Version" content="8.0.3612">		<meta name="Date" content="2/24/97">		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body text="black" bgcolor="white" link="blue" vlink="purple">		<h3><font size="4" face="Times New Roman,Georgia,Times"><b>What Next...?</b></font></h3>		<p><font face="Times New Roman">At this point, some conclusions can be pointed out.</font></p>		<p><font face="Times New Roman">The applications that were developed, were always that of feed-forward nets, with no memory. This was a way of keeping the learning task of the net, rather simple, and so, little time-consuming. As can be seen, this kind of network lacks the ability to learn audio evolution over time, with the consequence, of not displaying an audio coherent behavior on the output. The erratic nature of successive samples, results in perceived noise, severely limiting success.</font></p>		<p><font face="Times New Roman">Nevertheless, not only weird sound generation was (slightly) accomplished, or understanding of processes and respective limitations was gained, from the development of these instruments. Much work has been done in finding plausible applications, and adequate strategies for future projects.</font></p>		<p><font face="Times New Roman">I havent considered recurrent nets yet, due to the complexity and computational expense of that kind of networks. To learn time evolution of audio-rate data, we need massive networks, running for ages. This is rather suicidal to attempt using Csound. One promising solution resides in compressing the audio data, by means of FFT or Wavelet transformation, in order to present the net with not so complex patterns. I'm seriously considering those possibilities. Additionally, recurrent nets have trouble when dealing with long term dependencies (the learning of temporal evolution over a wide time interval), and although some recent improvements (eg, NARX networks) have come forward, it is still a very complex matter. If we restrict to learning MIDI information, much less complicated and limited in range, maybe by using simple networks for each specific task, there are some possibilities. I dont know if we can afford to run that inside Csound, but maybe a Csound utility or a highly optimized neural ugen could unleash that power to Csound users. It is not that impracticable.</font></p>		<p><font face="Times New Roman">We must make a compromise between big powerful nets running for ages, and smaller, simple task oriented, nets, which run fast and can be much more useful. These smaller nets, can be used to control k-rate variables, to perform slow tremolo or vibrato, or any other light-weighted task. We may for instance be able to train a net to perform physical instrument modeling, in impersonating a characteristic type of vibrato, or other slow rate features. It shouldnt be too hard to mimic a theremin, for instance, provided one has some suitable samples of the real thing to do the training... They could also be used to deal with MIDI data (since the range 0 to 127 is well within network reach). Things like algorithmic composition, where you use midi files, or even your own playing, as training material, may be possible in the future.</font></p>		<p><font face="Times New Roman">There's also a possibility of separating the learning and testing processes. The pain resides in the training, but after we've found a solution, it reduces itself to the actual weights of the net. As we've seen, this is an array of size H*(I+O) for the single hidden layer net, which could be written to disk, after training, and later read by a simpler instrument which just had the network response routines, allowing for a much faster performance. But this cant be feasible if we're training an independent net for each k-rate cycle, since a second of audio could mean a dump of several thousands of weight values to disk, something we're not willing to endure. Again, we're faced with the audio encoding problem.</font></p>		<p><font face="Times New Roman">Maybe the future of this line of work lies in the design of neural ugens to be used within Csound. One slow ugen could train the net and save the results, while another fast ugen read the solution and applied it to other audio sources, maybe opening the way to real-time processing. Maybe a Csound utility could be designed for this purpose as well, leaving the painful part out of Csound performance altogether. I am considering these possibilities, and you may hear from me again.</font></p>		<p><font face="Times New Roman">This approach might allow for recurrent networks to be used, and eventually, some kind of audio learning over time to be accomplished... This remains my main ambition, as far as this line of instruments is concerned, and it will be the core of my next projects.</font></p>		<p><font face="Times New Roman">		<hr>		</font></p>		<dir><font face="Times New Roman"><b>Next : </b><a href="links.html">Reference and links</a></font>			<p><font face="Times New Roman"><b>Previous : </b><a href="custom.html">A custom neural instrument</a></font></p>			<p><font face="Times New Roman"><b>Up:</b> <a href="index.html">Back to index</a></font>		</dir>	</body></html>