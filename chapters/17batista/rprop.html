<html>	<head>		<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">		<meta name="generator" content="Adobe GoLive 4">		<title>Neural Processing in Csound [ RProp ]</title>		<meta name="Version" content="8.0.3612">		<meta name="Date" content="2/24/97">		<meta name="Template" content="C:\PROGRAMAS\MICROSOFT OFFICE\OFFICE\HTML.DOT">	</head>	<body text="black" bgcolor="white" link="blue" vlink="purple">		<h3><font size="4" face="Times New Roman,Georgia,Times"><b>Resilient Back-Propagation</b></font></h3>		<p><font face="Times New Roman">To overcome the limitations of the classical gradient descent technique, many variations on the back-prop algorithm have been proposed. Some of the most well known are Quickprop, SuperSAB and RProp. All this methods present a reduction on the number of iterations needed to complete the task, as opposed to plain back-propagation, and manage to succeed in tasks where plain backprop fails. The most effective approaches use adaptive techniques, to correct the algorithm as the training proceeds. While backprop was originally a non-evolving gradient descent technique, where all customizing was decided at the beginning and then left unchanged during the training, this made it rather stiff when dealing with situations that required a more refined treatment. Consequently, adaptive methods that dynamically corrected the learning parameters (for instance the learning rate, the neurons activation thresholds, or the error derivative size, among others), started to emerge. One of my favorites is RProp, due to its simplicity, and powerful results.</font></p>		<p><font face="Times New Roman">As you recall, backpropagation tries to minimize the error gradient, by correcting the network weights, over consecutive iterations. At each iteration, the weight correction leaps the gradient to another value, which might close down to a minimum that solves the problem, but can also leap over the minimum, if the leap size is too big (due to the size of the error derivative being too big), and miss the solution.</font></p>		<p><font face="Times New Roman">To solve this problem, Martin Riedmiller proposed an adaptive method, where at each iteration, only the gradient sign is taken into account to determine the direction in which the network should evolve. Furthermore, each individual weight change amount is determined dynamically, and changes as a result of the learning process. Notice that the weight change amount, is independent of the gradient value, which is only used to calculate the direction of change for that weight.</font></p>		<p><font face="Times New Roman">I wont discuss the intricacies of the algorithm here (its rather simple, and you can get a describing paper from Riedmiller himself through the links section), but in words, what's being done is this: at each iteration, after all patterns have been presented to the net, and the error gradient for each weight has been computed, we compare each weight's gradient, with the preceding (from the previous iteration) gradient. If the derivative has changed its sign, then this means the minimum was leaped over, due to a too big step, so we must go back to the last iteration and try again with a smaller step. If on the contrary, the derivative has not changed its sign, then this means the algorithm is going down the hill, and the weight change amount should be increased, to speed up convergence.</font></p>		<h3><font size="4" face="Times New Roman,Georgia,Times"><b>Computing the Gradient</b></font></h3>		<p><font face="Times New Roman">Now, we'll be focusing on the gradient calculation code. In the backprop section we used a <i>delta</i> variable, since we didnt need separate <i><b>dE/do</b></i> and <i><b>dE/dw</b></i> values. For RProp we'll be needing the <i><b>dE/dw</b></i> values, which in turn need <i><b>dE/do</b></i> to be calculated, so we must compute them separately into their own arrays that you'll see below. We will also need another array for each weight's current increment value, as well as another for each weight's previous gradient value.</font></p>		<p><font face="Times New Roman">From now on we'll be considering a network with a single hidden layer. This may seem restrictive, but I think its a reasonable compromise, between network performance and processing time. Besides, most conventional functions do not need any extra hidden layers, so we'll be prepared for most problems.</font></p>		<p><font face="Times New Roman">Lets use the formulas for the calculation of the partial error derivatives, to write the gradient determination code (using informal C style). We'll focus on the procedure for a single pattern, not forgetting this process must be repeated for all patterns in an epoch.<br>		</font><font face="Times New Roman">&nbsp;</font></p>		<p><font face="Times New Roman"><i><b>Constants:</b></i></font></p>		<p><font face="Times New Roman">These determine the network topology,</font></p>		<p><font face="Times New Roman">		<table border="0" cellspacing="0" cellpadding="4" width="311">			<tr>				<td valign="top" width="36%">INSIZE</td>				<td valign="top" width="64%">number of input cells</td>			</tr>			<tr>				<td valign="top" width="36%">HIDSIZE</td>				<td valign="top" width="64%">number of hidden cells</td>			</tr>			<tr>				<td valign="top" width="36%">OUTSIZE</td>				<td valign="top" width="64%">number of output cells</td>			</tr>		</table>		&nbsp;</font></p>		<p><font face="Times New Roman"><i><b>Arrays:</b></i></font></p>		<p><font face="Times New Roman">These are the array variables we'll be needing to store the network values,</font></p>		<p><font face="Times New Roman">		<table border="0" cellspacing="0" cellpadding="4" width="682">			<tr>				<td valign="top" width="19%">a[L][I]</td>				<td valign="top" width="81%">activation of cell I in layer L</td>			</tr>			<tr>				<td valign="top" width="19%">w[L][I][J]</td>				<td valign="top" width="81%">weight of connection between cell I in layer L-1 (preceding layer) to cell J in layer L</td>			</tr>			<tr>				<td valign="top" width="19%">dedo[L][I]</td>				<td valign="top" width="81%">dE/do for cell I in layer L</td>			</tr>			<tr>				<td valign="top" width="19%">dedw[L][I][J]</td>				<td valign="top" width="81%">dE/dw for weight of connection between cell I in layer L-1 to cell J in layer L</td>			</tr>			<tr>				<td valign="top" width="19%">input[I]</td>				<td valign="top" width="81%">value number I of input pattern</td>			</tr>			<tr>				<td valign="top" width="19%">target[I]</td>				<td valign="top" width="81%">value number I of target pattern</td>			</tr>		</table>		</font></p>		<dir><font face="Times New Roman">&nbsp;</font>		</dir>		<div align="left">			<p><font face="Times New Roman">For simplicity, in this code skeleton, the layer index will be 'HID' when referring to the hidden layer, and 'OUT' when referring to the output layer. Since we'll be using the zak system later on, and also for clarity, all array values will be placed in local variables prior to computing result values.</font></p>		</div>		<p><font face="Times New Roman">One final remark, since the activation function is considered sigmoid, all the <i>f'(net) </i>terms will resolve to <i>out(1-out)</i></font><font face="Times New Roman">.&nbsp;</font></p>		<p><font face="Times New Roman"><i><b>Compute Gradient for output layer</b></i></font></p>		<p><font face="Times New Roman">		<table border="0" cellspacing="0" cellpadding="4" width="636">			<tr>				<td valign="top" colspan="3">for(k=0; k&lt;OUTSIZE; ++k) {</td>				<td valign="top" width="35%">&nbsp;</td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">tk = target[k]</td>				<td rowspan="3" width="35%"><img src="figures/form16.gif" height="59" width="171"></td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">ok = a[OUT][k]</td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">dedo[OUT][k] = -(tk-ok)</td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">for(j=0; j&lt;HIDSIZE; ++j) {</td>				<td rowspan="6" width="35%"><img src="figures/form15.gif" height="60" width="214"></td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="51%">dedok = dedo[OUT][k]</td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="51%">outk = a[OUT][k]</td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="51%">oj = a[HID][j]</td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="51%">dedw[OUT][j][k] += dedok*(outk*(1-outk))*oj</td>			</tr>			<tr>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">}</td>			</tr>			<tr>				<td valign="top" colspan="3">}</td>				<td valign="top" width="35%">&nbsp;</td>			</tr>		</table>		&nbsp;</font></p>		<p><font face="Times New Roman"><i><b>Compute Gradient for hidden layer</b></i></font></p>		<p><font face="Times New Roman">		<table border="0" cellspacing="0" cellpadding="4" width="656">			<tr>				<td valign="top" colspan="4">for(i=0; i&lt;INSIZE; i++) {</td>				<td valign="top" width="40%">&nbsp;</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" colspan="3">for(j=0; j&lt;HIDSIZE; j++) {</td>				<td valign="top" width="40%">&nbsp;</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">bp = 0</td>				<td rowspan="8" width="40%"><img src="figures/form18.gif" height="60" width="235"></td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">for(k=0; k&lt;OUTSIZE; ++k) {</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="39%">dedok = dedo[OUT][k]</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="39%">wjk = w[OUT][j][k]</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="39%">outk = a[OUT][k]</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" width="39%">bp += dedok*(outk*(1-outk))*wjk</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">}</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">dedo[HID][j] = bp</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">outj = a[HID][j]</td>				<td rowspan="4" width="40%"><img src="figures/form17.gif" height="60" width="207"></td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">oi = input[i]</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">dedoj = dedo[HID][j]</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" width="7%">&nbsp;</td>				<td valign="top" colspan="2">dedw[HID][i][j] += dedoj*(outj*(1-outj))*oi</td>			</tr>			<tr>				<td valign="top" width="6%">&nbsp;</td>				<td valign="top" colspan="3">}</td>				<td valign="top" width="40%">&nbsp;</td>			</tr>			<tr>				<td valign="top" colspan="4">}</td>				<td valign="top" width="40%">&nbsp;</td>			</tr>		</table>		&nbsp;</font></p>		<p><font face="Times New Roman">So, after this is done, the accumulated error gradient for this last pattern, is placed in the <i>dedw</i> array. We are now ready to transport this algorithm to Csound language, and apply the RProp weight correction scheme. That's what we'll do in the next section.&nbsp;&nbsp;</font></p>		<p><font face="Times New Roman">		<hr>		</font></p>		<dir><font face="Times New Roman"><b>Next : </b><a href="encoder.html">Testing RProp with a tight encoder</a></font>			<p><font face="Times New Roman"><b>Previous : </b><a href="bpxor.html">Testing back-prop with the XOR problem</a></font></p>			<p><font face="Times New Roman"><b>Up:</b> <a href="index.html">Back to index</a></font>		</dir>	</body></html>